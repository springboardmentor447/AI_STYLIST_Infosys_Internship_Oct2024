{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor447/AI_STYLIST_Infosys_Internship_Oct2024/blob/sainath-dobbali/Task_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad5881ad-4b6f-4df9-8443-b74060ad717f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad5881ad-4b6f-4df9-8443-b74060ad717f",
        "outputId": "59535f67-afa6-427e-e236-96ad82e2a5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDJSON file has been converted to CSV successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the LDJSON file\n",
        "ldjson_file = '/content/fashion_products_data (5).ldjson'  # replace with your actual file path\n",
        "data = pd.read_json(ldjson_file, lines=True)\n",
        "\n",
        "# Convert to CSV\n",
        "csv_file = 'fashion_products_data.csv'  # specify the output file name\n",
        "data.to_csv(csv_file, index=False)\n",
        "\n",
        "print(\"LDJSON file has been converted to CSV successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82029ae7-2a54-47d1-9599-72e07864f39e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82029ae7-2a54-47d1-9599-72e07864f39e",
        "outputId": "163a28ba-2731-41f8-e9fe-836b91472268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Basic Dataset Information ---\n",
            "\n",
            "1. Column Names:\n",
            " ['uniq_id', 'crawl_timestamp', 'asin', 'product_url', 'product_name', 'image_urls__small', 'medium', 'large', 'browsenode', 'brand', 'sales_price', 'weight', 'rating', 'sales_rank_in_parent_category', 'sales_rank_in_child_category', 'delivery_type', 'meta_keywords', 'amazon_prime__y_or_n', 'parent___child_category__all', 'best_seller_tag__y_or_n', 'other_items_customers_buy', 'product_details__k_v_pairs', 'discount_percentage', 'colour', 'no__of_reviews', 'seller_name', 'seller_id', 'left_in_stock', 'no__of_offers', 'no__of_sellers', 'technical_details__k_v_pairs', 'formats___editions', 'name_of_author_for_books']\n",
            "\n",
            "2. Data Types:\n",
            " uniq_id                          object\n",
            "crawl_timestamp                  object\n",
            "asin                             object\n",
            "product_url                      object\n",
            "product_name                     object\n",
            "image_urls__small                object\n",
            "medium                           object\n",
            "large                            object\n",
            "browsenode                       object\n",
            "brand                            object\n",
            "sales_price                      object\n",
            "weight                           object\n",
            "rating                           object\n",
            "sales_rank_in_parent_category    object\n",
            "sales_rank_in_child_category     object\n",
            "delivery_type                    object\n",
            "meta_keywords                    object\n",
            "amazon_prime__y_or_n             object\n",
            "parent___child_category__all     object\n",
            "best_seller_tag__y_or_n          object\n",
            "other_items_customers_buy        object\n",
            "product_details__k_v_pairs       object\n",
            "discount_percentage              object\n",
            "colour                           object\n",
            "no__of_reviews                   object\n",
            "seller_name                      object\n",
            "seller_id                        object\n",
            "left_in_stock                    object\n",
            "no__of_offers                    object\n",
            "no__of_sellers                   object\n",
            "technical_details__k_v_pairs     object\n",
            "formats___editions               object\n",
            "name_of_author_for_books         object\n",
            "dtype: object\n",
            "\n",
            "3. Shape of the Dataset:\n",
            " (30000, 33)\n",
            "\n",
            "4. Count of Non-Null Values:\n",
            " uniq_id                          30000\n",
            "crawl_timestamp                  30000\n",
            "asin                             30000\n",
            "product_url                      30000\n",
            "product_name                     30000\n",
            "image_urls__small                29998\n",
            "medium                           29998\n",
            "large                            28841\n",
            "browsenode                       29480\n",
            "brand                            21857\n",
            "sales_price                      27110\n",
            "weight                           30000\n",
            "rating                           30000\n",
            "sales_rank_in_parent_category    25497\n",
            "sales_rank_in_child_category     24851\n",
            "delivery_type                    30000\n",
            "meta_keywords                    30000\n",
            "amazon_prime__y_or_n             30000\n",
            "parent___child_category__all     25497\n",
            "best_seller_tag__y_or_n          30000\n",
            "other_items_customers_buy        24363\n",
            "product_details__k_v_pairs       28817\n",
            "discount_percentage              14624\n",
            "colour                            6029\n",
            "no__of_reviews                    3452\n",
            "seller_name                       8364\n",
            "seller_id                         8364\n",
            "left_in_stock                     3057\n",
            "no__of_offers                     1020\n",
            "no__of_sellers                    1020\n",
            "technical_details__k_v_pairs      1154\n",
            "formats___editions                   2\n",
            "name_of_author_for_books             1\n",
            "dtype: int64\n",
            "\n",
            "5. Null Value Count:\n",
            " uniq_id                              0\n",
            "crawl_timestamp                      0\n",
            "asin                                 0\n",
            "product_url                          0\n",
            "product_name                         0\n",
            "image_urls__small                    2\n",
            "medium                               2\n",
            "large                             1159\n",
            "browsenode                         520\n",
            "brand                             8143\n",
            "sales_price                       2890\n",
            "weight                               0\n",
            "rating                               0\n",
            "sales_rank_in_parent_category     4503\n",
            "sales_rank_in_child_category      5149\n",
            "delivery_type                        0\n",
            "meta_keywords                        0\n",
            "amazon_prime__y_or_n                 0\n",
            "parent___child_category__all      4503\n",
            "best_seller_tag__y_or_n              0\n",
            "other_items_customers_buy         5637\n",
            "product_details__k_v_pairs        1183\n",
            "discount_percentage              15376\n",
            "colour                           23971\n",
            "no__of_reviews                   26548\n",
            "seller_name                      21636\n",
            "seller_id                        21636\n",
            "left_in_stock                    26943\n",
            "no__of_offers                    28980\n",
            "no__of_sellers                   28980\n",
            "technical_details__k_v_pairs     28846\n",
            "formats___editions               29998\n",
            "name_of_author_for_books         29999\n",
            "dtype: int64\n",
            "\n",
            "6. Number of Unique Values in Each Column (Processed):\n",
            "   uniq_id: 30000\n",
            "   crawl_timestamp: 26645\n",
            "   asin: 29529\n",
            "   product_url: 30000\n",
            "   product_name: 22424\n",
            "   image_urls__small: 29477\n",
            "   medium: 29473\n",
            "   large: 28298\n",
            "   browsenode: 302\n",
            "   brand: 6458\n",
            "   sales_price: 2565\n",
            "   weight: 229\n",
            "   rating: 39\n",
            "   sales_rank_in_parent_category: Cannot calculate unique values (unhashable type)\n",
            "   sales_rank_in_child_category: Cannot calculate unique values (unhashable type)\n",
            "   delivery_type: 2\n",
            "   meta_keywords: 24664\n",
            "   amazon_prime__y_or_n: 2\n",
            "   parent___child_category__all: Cannot calculate unique values (unhashable type)\n",
            "   best_seller_tag__y_or_n: 2\n",
            "   other_items_customers_buy: 23922\n",
            "   product_details__k_v_pairs: Cannot calculate unique values (unhashable type)\n",
            "   discount_percentage: 98\n",
            "   colour: 4757\n",
            "   no__of_reviews: 461\n",
            "   seller_name: 2333\n",
            "   seller_id: 2333\n",
            "   left_in_stock: 5\n",
            "   no__of_offers: 54\n",
            "   no__of_sellers: 54\n",
            "   technical_details__k_v_pairs: Cannot calculate unique values (unhashable type)\n",
            "   formats___editions: 2\n",
            "   name_of_author_for_books: 1\n",
            "\n",
            "7. Sample Data:\n",
            "                             uniq_id            crawl_timestamp        asin  \\\n",
            "0  26d41bdc1495de290bc8e6062d927729  2020-02-07 05:11:36 +0000  B07STS2W9T   \n",
            "1  410c62298852e68f34c35560f2311e5a  2020-02-07 08:45:56 +0000  B07N6TD2WL   \n",
            "2  52e31bb31680b0ec73de0d781a23cc0a  2020-02-06 11:09:38 +0000  B07WJ6WPN1   \n",
            "3  25798d6dc43239c118452d1bee0fb088  2020-02-07 08:32:45 +0000  B07PYSF4WZ   \n",
            "4  ad8a5a196d515ef09dfdaf082bdc37c4  2020-02-06 14:27:48 +0000  B082KXNM7X   \n",
            "\n",
            "                                         product_url  \\\n",
            "0  https://www.amazon.in/Facon-Kalamkari-Handbloc...   \n",
            "1  https://www.amazon.in/Sf-Jeans-Pantaloons-T-Sh...   \n",
            "2  https://www.amazon.in/LOVISTA-Traditional-Prin...   \n",
            "3  https://www.amazon.in/People-Printed-Regular-T...   \n",
            "4  https://www.amazon.in/Monte-Carlo-Cotton-Colla...   \n",
            "\n",
            "                                        product_name  \\\n",
            "0  LA' Facon Cotton Kalamkari Handblock Saree Blo...   \n",
            "1  Sf Jeans By Pantaloons Men's Plain Slim fit T-...   \n",
            "2  LOVISTA Cotton Gota Patti Tassel Traditional P...   \n",
            "3           People Men's Printed Regular fit T-Shirt   \n",
            "4  Monte Carlo Grey Solid Cotton Blend Polo Colla...   \n",
            "\n",
            "                                   image_urls__small  \\\n",
            "0  https://images-na.ssl-images-amazon.com/images...   \n",
            "1  https://images-na.ssl-images-amazon.com/images...   \n",
            "2  https://images-na.ssl-images-amazon.com/images...   \n",
            "3  https://images-na.ssl-images-amazon.com/images...   \n",
            "4  https://images-na.ssl-images-amazon.com/images...   \n",
            "\n",
            "                                              medium  \\\n",
            "0  https://images-na.ssl-images-amazon.com/images...   \n",
            "1  https://images-na.ssl-images-amazon.com/images...   \n",
            "2  https://images-na.ssl-images-amazon.com/images...   \n",
            "3  https://images-na.ssl-images-amazon.com/images...   \n",
            "4  https://images-na.ssl-images-amazon.com/images...   \n",
            "\n",
            "                                               large  browsenode      brand  \\\n",
            "0  https://images-na.ssl-images-amazon.com/images...  1968255031  LA' Facon   \n",
            "1  https://images-na.ssl-images-amazon.com/images...  1968123031        NaN   \n",
            "2  https://images-na.ssl-images-amazon.com/images...  1968255031    LOVISTA   \n",
            "3  https://images-na.ssl-images-amazon.com/images...  1968123031        NaN   \n",
            "4  https://images-na.ssl-images-amazon.com/images...  1968070031        NaN   \n",
            "\n",
            "   ... colour no__of_reviews seller_name seller_id left_in_stock  \\\n",
            "0  ...    NaN            NaN         NaN       NaN           NaN   \n",
            "1  ...    NaN            NaN         NaN       NaN           NaN   \n",
            "2  ...    NaN            NaN         NaN       NaN           NaN   \n",
            "3  ...    NaN            NaN         NaN       NaN           NaN   \n",
            "4  ...    NaN            NaN         NaN       NaN           NaN   \n",
            "\n",
            "  no__of_offers no__of_sellers technical_details__k_v_pairs  \\\n",
            "0           NaN            NaN                          NaN   \n",
            "1           NaN            NaN                          NaN   \n",
            "2           NaN            NaN                          NaN   \n",
            "3           NaN            NaN                          NaN   \n",
            "4           NaN            NaN                          NaN   \n",
            "\n",
            "  formats___editions name_of_author_for_books  \n",
            "0                NaN                      NaN  \n",
            "1                NaN                      NaN  \n",
            "2                NaN                      NaN  \n",
            "3                NaN                      NaN  \n",
            "4                NaN                      NaN  \n",
            "\n",
            "[5 rows x 33 columns]\n",
            "\n",
            "8. Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30000 entries, 0 to 29999\n",
            "Data columns (total 33 columns):\n",
            " #   Column                         Non-Null Count  Dtype \n",
            "---  ------                         --------------  ----- \n",
            " 0   uniq_id                        30000 non-null  object\n",
            " 1   crawl_timestamp                30000 non-null  object\n",
            " 2   asin                           30000 non-null  object\n",
            " 3   product_url                    30000 non-null  object\n",
            " 4   product_name                   30000 non-null  object\n",
            " 5   image_urls__small              29998 non-null  object\n",
            " 6   medium                         29998 non-null  object\n",
            " 7   large                          28841 non-null  object\n",
            " 8   browsenode                     29480 non-null  object\n",
            " 9   brand                          21857 non-null  object\n",
            " 10  sales_price                    27110 non-null  object\n",
            " 11  weight                         30000 non-null  object\n",
            " 12  rating                         30000 non-null  object\n",
            " 13  sales_rank_in_parent_category  25497 non-null  object\n",
            " 14  sales_rank_in_child_category   24851 non-null  object\n",
            " 15  delivery_type                  30000 non-null  object\n",
            " 16  meta_keywords                  30000 non-null  object\n",
            " 17  amazon_prime__y_or_n           30000 non-null  object\n",
            " 18  parent___child_category__all   25497 non-null  object\n",
            " 19  best_seller_tag__y_or_n        30000 non-null  object\n",
            " 20  other_items_customers_buy      24363 non-null  object\n",
            " 21  product_details__k_v_pairs     28817 non-null  object\n",
            " 22  discount_percentage            14624 non-null  object\n",
            " 23  colour                         6029 non-null   object\n",
            " 24  no__of_reviews                 3452 non-null   object\n",
            " 25  seller_name                    8364 non-null   object\n",
            " 26  seller_id                      8364 non-null   object\n",
            " 27  left_in_stock                  3057 non-null   object\n",
            " 28  no__of_offers                  1020 non-null   object\n",
            " 29  no__of_sellers                 1020 non-null   object\n",
            " 30  technical_details__k_v_pairs   1154 non-null   object\n",
            " 31  formats___editions             2 non-null      object\n",
            " 32  name_of_author_for_books       1 non-null      object\n",
            "dtypes: object(33)\n",
            "memory usage: 7.6+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Path to the uploaded data\n",
        "file_path = '/content/fashion_products_data (5).ldjson'  # Adjust this path if necessary\n",
        "\n",
        "# Load the data from the .ldjson file\n",
        "data = []\n",
        "\n",
        "# Open and read data line by line\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Convert data into a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to display dataset details\n",
        "def display_dataset_info(dataframe):\n",
        "    print(\"\\n--- Basic Dataset Information ---\\n\")\n",
        "    print(\"1. Column Names:\\n\", dataframe.columns.tolist())\n",
        "    print(\"\\n2. Data Types:\\n\", dataframe.dtypes)\n",
        "    print(\"\\n3. Shape of the Dataset:\\n\", dataframe.shape)\n",
        "    print(\"\\n4. Count of Non-Null Values:\\n\", dataframe.count())\n",
        "    print(\"\\n5. Null Value Count:\\n\", dataframe.isnull().sum())\n",
        "\n",
        "    print(\"\\n6. Number of Unique Values in Each Column (Processed):\")\n",
        "    for col in dataframe.columns:\n",
        "        try:\n",
        "            unique_count = dataframe[col].nunique()\n",
        "            print(f\"   {col}: {unique_count}\")\n",
        "        except TypeError:\n",
        "            print(f\"   {col}: Cannot calculate unique values (unhashable type)\")\n",
        "\n",
        "    print(\"\\n7. Sample Data:\\n\", dataframe.head())\n",
        "    print(\"\\n8. Dataset Info:\")\n",
        "    dataframe.info()\n",
        "\n",
        "# Display dataset details once\n",
        "display_dataset_info(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d47d97-4cad-41ca-a586-99929c2e9250",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46d47d97-4cad-41ca-a586-99929c2e9250",
        "outputId": "4e51da27-fe7d-4822-faa2-0c8120f37759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available column names in the dataset:\n",
            "'uniq_id'\n",
            "'crawl_timestamp'\n",
            "'asin'\n",
            "'product_url'\n",
            "'product_name'\n",
            "'image_urls__small'\n",
            "'medium'\n",
            "'large'\n",
            "'browsenode'\n",
            "'brand'\n",
            "'sales_price'\n",
            "'weight'\n",
            "'rating'\n",
            "'sales_rank_in_parent_category'\n",
            "'sales_rank_in_child_category'\n",
            "'delivery_type'\n",
            "'meta_keywords'\n",
            "'amazon_prime__y_or_n'\n",
            "'parent___child_category__all'\n",
            "'best_seller_tag__y_or_n'\n",
            "'other_items_customers_buy'\n",
            "'product_details__k_v_pairs'\n",
            "'discount_percentage'\n",
            "'colour'\n",
            "'no__of_reviews'\n",
            "'seller_name'\n",
            "'seller_id'\n",
            "'left_in_stock'\n",
            "'no__of_offers'\n",
            "'no__of_sellers'\n",
            "'technical_details__k_v_pairs'\n",
            "'formats___editions'\n",
            "'name_of_author_for_books'\n",
            "\n",
            "Columns after potential renaming:\n",
            "Index(['uniq_id', 'crawl_timestamp', 'asin', 'product_url', 'product_name',\n",
            "       'image_urls__small', 'medium', 'large', 'browsenode', 'brand',\n",
            "       'sales_price', 'weight', 'rating', 'sales_rank_in_parent_category',\n",
            "       'sales_rank_in_child_category', 'delivery_type', 'meta_keywords',\n",
            "       'amazon_prime__y_or_n', 'parent___child_category__all',\n",
            "       'best_seller_tag__y_or_n', 'other_items_customers_buy',\n",
            "       'product_details__k_v_pairs', 'discount_percentage', 'colour',\n",
            "       'no__of_reviews', 'seller_name', 'seller_id', 'left_in_stock',\n",
            "       'no__of_offers', 'no__of_sellers', 'technical_details__k_v_pairs',\n",
            "       'formats___editions', 'name_of_author_for_books'],\n",
            "      dtype='object')\n",
            "Warning: 'image_url' column not found. Images will not be displayed.\n",
            "\n",
            "Sample data from the dataset:\n",
            "                                        product_name image_url\n",
            "0  LA' Facon Cotton Kalamkari Handblock Saree Blo...          \n",
            "1  Sf Jeans By Pantaloons Men's Plain Slim fit T-...          \n",
            "2  LOVISTA Cotton Gota Patti Tassel Traditional P...          \n",
            "3           People Men's Printed Regular fit T-Shirt          \n",
            "4  Monte Carlo Grey Solid Cotton Blend Polo Colla...          \n",
            "Product 'prakasam cotton men cotten dhoti' not found in the dataset.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import display, Image\n",
        "import os\n",
        "import json\n",
        "# Load the dataset\n",
        "file_path = '/content/fashion_products_data (5).ldjson'  # Adjust this path if necessary\n",
        "data = []\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Debug: Print available columns\n",
        "print(\"Available column names in the dataset:\")\n",
        "for col in df.columns:\n",
        "    print(f\"'{col}'\")\n",
        "\n",
        "# Adjust column names if necessary\n",
        "if 'name' in df.columns:\n",
        "    df.rename(columns={'name': 'product_name'}, inplace=True)\n",
        "if 'image_link' in df.columns:\n",
        "    df.rename(columns={'image_link': 'image_url'}, inplace=True)\n",
        "if 'image' in df.columns:\n",
        "    df.rename(columns={'image': 'image_url'}, inplace=True)\n",
        "\n",
        "# Debug: Check after renaming\n",
        "print(\"\\nColumns after potential renaming:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Check for required columns\n",
        "if 'product_name' not in df.columns:\n",
        "    print(\"Warning: 'product_name' column not found. Using a default name.\")\n",
        "    df['product_name'] = \"\"  # Fill with empty strings if missing\n",
        "if 'image_url' not in df.columns:\n",
        "    print(\"Warning: 'image_url' column not found. Images will not be displayed.\")\n",
        "    df['image_url'] = \"\"  # Fill with empty strings if missing\n",
        "\n",
        "# Handle missing values\n",
        "df['product_name'] = df['product_name'].fillna('')\n",
        "df['image_url'] = df['image_url'].fillna('')\n",
        "\n",
        "# Debug: Display sample data\n",
        "print(\"\\nSample data from the dataset:\")\n",
        "print(df[['product_name', 'image_url']].head())\n",
        "\n",
        "# Use CountVectorizer for Bag-of-Words Model\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "product_vectors = vectorizer.fit_transform(df['product_name'])\n",
        "\n",
        "# Define the Recommendation Function\n",
        "def recommend_products(product_name, top_n=5):\n",
        "    # Find the index of the given product name\n",
        "    if product_name not in df['product_name'].values:\n",
        "        print(f\"Product '{product_name}' not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    product_idx = df[df['product_name'] == product_name].index[0]\n",
        "\n",
        "    # Compute cosine similarity between the given product and all other products\n",
        "    similarity_scores = cosine_similarity(product_vectors[product_idx], product_vectors).flatten()\n",
        "\n",
        "    # Get the indices of the top N most similar products (excluding the given product itself)\n",
        "    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]\n",
        "\n",
        "    # Display the recommended products and their images\n",
        "    print(f\"\\nRecommendations for '{product_name}':\\n\")\n",
        "    for idx in similar_indices:\n",
        "        recommended_name = df.iloc[idx]['product_name']\n",
        "        image_url = df.iloc[idx]['image_url']\n",
        "        print(f\"- {recommended_name}\")\n",
        "        if image_url and os.path.isfile(image_url):  # Ensure image URL is valid\n",
        "            display(Image(image_url))\n",
        "        else:\n",
        "            print(\"(No image available or invalid URL)\")\n",
        "\n",
        "# Example: Recommend products similar to a given product name\n",
        "input_product = \"prakasam cotton men cotten dhoti\"\n",
        "# Replace with a valid product name from your dataset\n",
        "recommend_products(input_product, top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda965fc-8668-452d-95a3-3bde34d4c076",
      "metadata": {
        "id": "eda965fc-8668-452d-95a3-3bde34d4c076",
        "outputId": "329b046c-3ccb-41f3-c551-f84fa6aae6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available column names in the dataset:\n",
            "'uniq_id'\n",
            "'crawl_timestamp'\n",
            "'asin'\n",
            "'product_url'\n",
            "'product_name'\n",
            "'image_urls__small'\n",
            "'medium'\n",
            "'large'\n",
            "'browsenode'\n",
            "'brand'\n",
            "'sales_price'\n",
            "'weight'\n",
            "'rating'\n",
            "'sales_rank_in_parent_category'\n",
            "'sales_rank_in_child_category'\n",
            "'delivery_type'\n",
            "'meta_keywords'\n",
            "'amazon_prime__y_or_n'\n",
            "'parent___child_category__all'\n",
            "'best_seller_tag__y_or_n'\n",
            "'other_items_customers_buy'\n",
            "'product_details__k_v_pairs'\n",
            "'discount_percentage'\n",
            "'colour'\n",
            "'no__of_reviews'\n",
            "'seller_name'\n",
            "'seller_id'\n",
            "'left_in_stock'\n",
            "'no__of_offers'\n",
            "'no__of_sellers'\n",
            "'technical_details__k_v_pairs'\n",
            "'formats___editions'\n",
            "'name_of_author_for_books'\n",
            "\n",
            "Columns after potential renaming:\n",
            "Index(['uniq_id', 'crawl_timestamp', 'asin', 'product_url', 'product_name',\n",
            "       'image_url', 'medium', 'large', 'browsenode', 'brand', 'sales_price',\n",
            "       'weight', 'rating', 'sales_rank_in_parent_category',\n",
            "       'sales_rank_in_child_category', 'delivery_type', 'meta_keywords',\n",
            "       'amazon_prime__y_or_n', 'parent___child_category__all',\n",
            "       'best_seller_tag__y_or_n', 'other_items_customers_buy',\n",
            "       'product_details__k_v_pairs', 'discount_percentage', 'colour',\n",
            "       'no__of_reviews', 'seller_name', 'seller_id', 'left_in_stock',\n",
            "       'no__of_offers', 'no__of_sellers', 'technical_details__k_v_pairs',\n",
            "       'formats___editions', 'name_of_author_for_books'],\n",
            "      dtype='object')\n",
            "\n",
            "Sample data from the dataset:\n",
            "                                        product_name  \\\n",
            "0  LA' Facon Cotton Kalamkari Handblock Saree Blo...   \n",
            "1  Sf Jeans By Pantaloons Men's Plain Slim fit T-...   \n",
            "2  LOVISTA Cotton Gota Patti Tassel Traditional P...   \n",
            "3           People Men's Printed Regular fit T-Shirt   \n",
            "4  Monte Carlo Grey Solid Cotton Blend Polo Colla...   \n",
            "\n",
            "                                           image_url  \n",
            "0  https://images-na.ssl-images-amazon.com/images...  \n",
            "1  https://images-na.ssl-images-amazon.com/images...  \n",
            "2  https://images-na.ssl-images-amazon.com/images...  \n",
            "3  https://images-na.ssl-images-amazon.com/images...  \n",
            "4  https://images-na.ssl-images-amazon.com/images...  \n",
            "\n",
            "Sample product names in the dataset:\n",
            "0    LA' Facon Cotton Kalamkari Handblock Saree Blo...\n",
            "1    Sf Jeans By Pantaloons Men's Plain Slim fit T-...\n",
            "2    LOVISTA Cotton Gota Patti Tassel Traditional P...\n",
            "3             People Men's Printed Regular fit T-Shirt\n",
            "4    Monte Carlo Grey Solid Cotton Blend Polo Colla...\n",
            "5    Forest Club | Gym Wear | Sports Shorts| Shorts...\n",
            "6    PrintOctopus Graphic Printed T-Shirt for Men C...\n",
            "7      Pepe Jeans Men's Solid Regular fit Casual Shirt\n",
            "8    Carahere Boys Handmade Pre-Tied Classic Polka ...\n",
            "9                           Peppermint Synthetic Dress\n",
            "Name: product_name, dtype: object\n",
            "Product 'la' facon cotton kalamkari handblock saree blouse piece' not found in the dataset.\n",
            "\n",
            "Unique product names in the dataset:\n",
            "[\"LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\"\n",
            " \"Sf Jeans By Pantaloons Men's Plain Slim fit T-Shirt\"\n",
            " 'LOVISTA Cotton Gota Patti Tassel Traditional Printed Kurti with Pant,Angrakha Salwar Suit'\n",
            " \"People Men's Printed Regular fit T-Shirt\"\n",
            " 'Monte Carlo Grey Solid Cotton Blend Polo Collar Tracksuit'\n",
            " 'Forest Club | Gym Wear | Sports Shorts| Shorts for Men | Smooth Breathable Fabric | Shorts with Pocket Zippers | All TIME WEAR |'\n",
            " 'PrintOctopus Graphic Printed T-Shirt for Men Chill T-Shirt | Hindi Quote T-Shirt | Half Sleeve T-Shirt for Women | Round Neck T Shirt | 100% Cotton T-Shirt | Short Sleeve T Shirt'\n",
            " \"Pepe Jeans Men's Solid Regular fit Casual Shirt\"\n",
            " 'Carahere Boys Handmade Pre-Tied Classic Polka Dot Bow Ties M132 Navy Blue'\n",
            " 'Peppermint Synthetic Dress'\n",
            " 'Toddler Little Boy Straight Outta Timeout Long Sleeve T-Shirt (2T, Black)'\n",
            " 'Puma Unisex Tribal Regular Fit T-Shirt'\n",
            " \"Jevi Prints Women's Cotton Printed Straight Kurta (PRJ-617)\"\n",
            " \"OLLI Men's Orange, Lex Purple, White Cotton Brief Innerwear (Pack of 3)\"\n",
            " \"Pinkmint Women's Multi-Coloured Digital Print Crepe Kurti\"\n",
            " \"Miss Chase Women's Solid Shoulder Cut-Out Half-Sleeve Round Neck Mini Dresses\"\n",
            " 'bebe Women Genuine Leather Slim Belt' \"Colt by Unlimited Men's T-Shirt\"\n",
            " \"Pepe Jeans Women's Solid T-Shirt\" 'C9 Women Turquoise Active T-Shirt']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import display, Image\n",
        "import json\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/admin/fashion_products_data.ldjson'  # Adjust this path if necessary\n",
        "data = []\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Debug: Print available columns\n",
        "print(\"Available column names in the dataset:\")\n",
        "for col in df.columns:\n",
        "    print(f\"'{col}'\")\n",
        "\n",
        "# Adjust column names if necessary\n",
        "if 'name' in df.columns:\n",
        "    df.rename(columns={'name': 'product_name'}, inplace=True)\n",
        "\n",
        "# Choose the appropriate image column\n",
        "if 'image_urls__small' in df.columns:\n",
        "    df.rename(columns={'image_urls__small': 'image_url'}, inplace=True)\n",
        "elif 'medium' in df.columns:\n",
        "    df.rename(columns={'medium': 'image_url'}, inplace=True)\n",
        "elif 'large' in df.columns:\n",
        "    df.rename(columns={'large': 'image_url'}, inplace=True)\n",
        "else:\n",
        "    print(\"Warning: No valid image column found. Images will not be displayed.\")\n",
        "    df['image_url'] = \"\"  # Placeholder if no image column exists\n",
        "\n",
        "# Debug: Check after renaming\n",
        "print(\"\\nColumns after potential renaming:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Handle missing values\n",
        "df['product_name'] = df['product_name'].fillna('')\n",
        "df['image_url'] = df['image_url'].fillna('')\n",
        "\n",
        "# Debug: Display sample data\n",
        "print(\"\\nSample data from the dataset:\")\n",
        "print(df[['product_name', 'image_url']].head())\n",
        "\n",
        "# Use CountVectorizer for Bag-of-Words Model\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "product_vectors = vectorizer.fit_transform(df['product_name'])\n",
        "\n",
        "# Debug: Print sample product names\n",
        "print(\"\\nSample product names in the dataset:\")\n",
        "print(df['product_name'].head(10))\n",
        "\n",
        "# Define the Recommendation Function with flexible matching\n",
        "def recommend_products(product_name, top_n=5):\n",
        "    # Make the product_name lowercase for case-insensitive comparison\n",
        "    product_name = product_name.lower()\n",
        "\n",
        "    # Search for the product name in a case-insensitive manner\n",
        "    matched_idx = df[df['product_name'].str.lower().str.contains(product_name)].index\n",
        "\n",
        "    if len(matched_idx) == 0:\n",
        "        print(f\"Product '{product_name}' not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Take the first match (or you could handle multiple results)\n",
        "    product_idx = matched_idx[0]\n",
        "\n",
        "    # Compute cosine similarity between the given product and all other products\n",
        "    similarity_scores = cosine_similarity(product_vectors[product_idx], product_vectors).flatten()\n",
        "\n",
        "    # Get the indices of the top N most similar products (excluding the given product itself)\n",
        "    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]\n",
        "\n",
        "    # Display the recommended products and their images\n",
        "    print(f\"\\nRecommendations for '{product_name}':\\n\")\n",
        "    for idx in similar_indices:\n",
        "        recommended_name = df.iloc[idx]['product_name']\n",
        "        image_url = df.iloc[idx]['image_url']\n",
        "        print(f\"- {recommended_name}\")\n",
        "        if image_url.startswith(\"http\"):\n",
        "            try:\n",
        "                display(Image(image_url))\n",
        "            except Exception as e:\n",
        "                print(\"(Image could not be displayed)\")\n",
        "        else:\n",
        "            print(\"(No valid image URL)\")\n",
        "\n",
        "# Example: Recommend products similar to a given product name\n",
        "input_product = \"LA' Facon Cotton Kalamkari Handblock Saree Blouse Piece\"\n",
        "recommend_products(input_product, top_n=5)\n",
        "\n",
        "# Optionally, you can test with other known products\n",
        "# recommend_products(\"Cotton Saree\", top_n=5)\n",
        "\n",
        "# Optionally, you can print unique product names to check for exact matches\n",
        "print(\"\\nUnique product names in the dataset:\")\n",
        "print(df['product_name'].unique()[:20])  # Display first 20 unique product names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d59dc28d-72c8-4491-9316-6c95d4ed5ef8",
      "metadata": {
        "id": "d59dc28d-72c8-4491-9316-6c95d4ed5ef8",
        "outputId": "69a7c2af-1b3e-467d-ce13-588afdf05adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset with vectors created successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Convert product names to vectors using CountVectorizer (Bag of Words)\n",
        "count_vect = CountVectorizer()\n",
        "vector_matrix = count_vect.fit_transform(data['product_name'])\n",
        "\n",
        "# Optional: Store vector matrix in the DataFrame (if needed)\n",
        "# Convert sparse matrix to dense for representation\n",
        "data['vector'] = list(vector_matrix.toarray())\n",
        "\n",
        "print(\"Dataset with vectors created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f30b559a-fb8f-4042-a8a0-e7d1c971ceb5",
      "metadata": {
        "id": "f30b559a-fb8f-4042-a8a0-e7d1c971ceb5",
        "outputId": "b1ad4281-5538-46c8-d413-a2de12632888"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Distance: 0.0\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/810yHP4dVtL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91Lk9W9EuoL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 2d249ce6637640fe9ef8a5cf20cbc9dc\n",
            "Product ID: B07W5X5BN4\n",
            "Brand Name: La' Facon\n",
            "Product Name: Cotton Kalamkari Handblock Saree Blouse/Kurti Fabric 100 cms Purple Colour - Budda Print\n",
            "Distance: 3.3166247903554\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/91st36UyiWL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81Q%2BR1J0MkL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91wvWkYspuL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91O-SEQsILL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91JsHErQ3hL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: f8f4bfd73774176d6b7af84cba15faa4\n",
            "Product ID: B079TL6TPT\n",
            "Brand Name: PERFECTBLUE\n",
            "Product Name: PERFECTBLUE Cotton with Blouse Piece Saree\n",
            "Distance: 3.7416573867739413\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/710Jx%2BJ3LwL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/713EPfjYqML._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/713EPfjYqML._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61j9m4Mqm4L._UL1171_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 8d74ba83812beb7a9f622d83f921be2d\n",
            "Product ID: B07RSKJPY7\n",
            "Brand Name: KanishaTrendz\n",
            "Product Name: Kanishatrendz Cotton Saree With Blouse Piece\n",
            "Distance: 3.7416573867739413\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81Z1qpcQR3L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81h3xLPRfIL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81RUXC0%2BOwL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81zKWgDOdiL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81pzHHQmo7L._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: b23a8a2ad0dfc51d7c1f80a49e3a669d\n",
            "Product ID: B07C6RQ9TD\n",
            "Brand Name: PERFECTBLUE\n",
            "Product Name: PerfectBlue Cotton Saree with Blouse Piece\n",
            "Distance: 3.7416573867739413\n",
            "____________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url'])\n",
        "\n",
        "# Convert product names to vectors using Bag of Words (BoW)\n",
        "count_vect = CountVectorizer()\n",
        "vector_matrix = count_vect.fit_transform(data_cleaned['product_name'])\n",
        "\n",
        "# Recommender System Function\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on textual similarity.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Sparse matrix of product vectors (BoW)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Compute pairwise distances using sparse data\n",
        "        dist = pairwise_distances(vectors, vectors[p_id], metric='euclidean')\n",
        "        # Sort distances\n",
        "        indices = np.argsort(dist.flatten())[1:numProduct + 1]  # Skip self\n",
        "        p_dist = np.sort(dist.flatten())[1:numProduct + 1]\n",
        "\n",
        "        similar_products = list(indices)\n",
        "\n",
        "        for i, idx in enumerate(similar_products):\n",
        "            # Display product image and details\n",
        "            display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "            print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "            print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "            print(f\"Brand Name: {dataset['brand_name'].iloc[idx] if not pd.isna(dataset['brand_name'].iloc[idx]) else 'Unknown'}\")\n",
        "            print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "            print(f\"Distance: {p_dist[i]}\")\n",
        "            print(\"_\" * 60)\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage: Pass a valid p_id (row index of a product in the cleaned DataFrame)\n",
        "RecommenderSystems(0, 5, vector_matrix, data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6704c789-43cb-4aab-bee0-3fcdb6241ee5",
      "metadata": {
        "id": "6704c789-43cb-4aab-bee0-3fcdb6241ee5",
        "outputId": "099812ac-b51e-44e5-ce29-93ce3f45034c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Distance: 0.0\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/810yHP4dVtL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91Lk9W9EuoL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 2d249ce6637640fe9ef8a5cf20cbc9dc\n",
            "Product ID: B07W5X5BN4\n",
            "Brand Name: La' Facon\n",
            "Product Name: Cotton Kalamkari Handblock Saree Blouse/Kurti Fabric 100 cms Purple Colour - Budda Print\n",
            "Distance: 0.46672432404068764\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/61xQchI2DZL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61KruNw3FUL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/612C0J8VSIL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61iv0ktviDL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71yuKf4rZqL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 2273dc9330251b629313b0830ceac78b\n",
            "Product ID: B07TKWSKVH\n",
            "Brand Name: HANDBLOCK PRINT\n",
            "Product Name: HANDBLOCK PRINT Women's Rayon Embroidered Anarkali Kurti\n",
            "Distance: 0.7625528222195675\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81N6Hg1iNRL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81q4g9s2BNL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91u4ZxZv24L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81wlJcyjkML._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: b160ddc37e03f11dc56410d7dbf0b9da\n",
            "Product ID: B07NY257RF\n",
            "Brand Name: Winza Designer\n",
            "Product Name: Winza Designer Women's Kalamkari Art Silk Saree With Blouse\n",
            "Distance: 0.7820209639190658\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81PrI7hu-xL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81Uq01bxtaL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71TWlmF4IBL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71jHif-1hzL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: d8f9a3fe8b80227a37b8c5529931f383\n",
            "Product ID: B07N57N5GK\n",
            "Brand Name: Generic\n",
            "Product Name: Kalamkari Print Cotton Fabric for Women's Clothes Like Kurti, Blouse, Dress, Patiala, Palazzo and Chudidar\n",
            "Distance: 0.7943880835975903\n",
            "____________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url'])\n",
        "\n",
        "# Convert product names to vectors using TF-IDF\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "vector_matrix = tfidf_vect.fit_transform(data_cleaned['product_name'])\n",
        "\n",
        "# Recommender System Function\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on textual similarity using TF-IDF.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Sparse matrix of product vectors (TF-IDF)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Compute pairwise distances using sparse data\n",
        "        dist = pairwise_distances(vectors, vectors[p_id], metric='cosine')  # Use cosine similarity\n",
        "        # Sort distances\n",
        "        indices = np.argsort(dist.flatten())[1:numProduct + 1]  # Skip self\n",
        "        p_dist = np.sort(dist.flatten())[1:numProduct + 1]\n",
        "\n",
        "        similar_products = list(indices)\n",
        "\n",
        "        for i, idx in enumerate(similar_products):\n",
        "            # Display product image and details\n",
        "            display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "            print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "            print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "            print(f\"Brand Name: {dataset['brand_name'].iloc[idx] if not pd.isna(dataset['brand_name'].iloc[idx]) else 'Unknown'}\")\n",
        "            print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "            print(f\"Distance: {p_dist[i]}\")\n",
        "            print(\"_\" * 60)\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage: Pass a valid p_id (row index of a product in the cleaned DataFrame)\n",
        "RecommenderSystems(0, 5, vector_matrix, data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6a2511-c76f-4146-96e0-920f03a075e8",
      "metadata": {
        "id": "6b6a2511-c76f-4146-96e0-920f03a075e8",
        "outputId": "9a96481d-825e-49ef-c568-613959a1dff4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Cosine Similarity: 1.0\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/617aBr0G5JL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61lgJ7Q0wEL._UL1477_.jpg|https://images-na.ssl-images-amazon.com/images/I/91kzuUNn-JL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91cdv0Ljp2L._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: c10790d6cc21e0f0a2a359e234622bef\n",
            "Product ID: B01MQYAG3O\n",
            "Brand Name: Bishwadip Basak\n",
            "Product Name: Bishwadip Basak Cotton Saree (Bishwadip 18_Orange)\n",
            "Cosine Similarity: 0.9924516677856445\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81ZUqT096uL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81SgZcDw0VL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81SVeJ%2BJLLL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 71e86a7f6944b24d7f3113e69cea4f11\n",
            "Product ID: B00QGQGWDG\n",
            "Brand Name: Romi's\n",
            "Product Name: Romi's Handpainted Cotton Saree (Orange)\n",
            "Cosine Similarity: 0.9922559857368469\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/71WVOSdNTvL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71KgbhJ2xCL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81qKjOnTHkL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81HeWea-wPL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1UmBw3OgrL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A14%2B7lx9PWL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1GP0J5SJtL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: dd5fb34f1242121aeff67c7d068b43fe\n",
            "Product ID: B07WNWLLWV\n",
            "Brand Name: Viva N Diva\n",
            "Product Name: Viva N Diva Off White Poly Cotton Blend Printed Saree with Blouse\n",
            "Cosine Similarity: 0.9912440776824951\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/818WKJV91yL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81hZZQoalGL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81uQ5tb0aCL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 3a6840343b71c25bd0ee1426355c2d28\n",
            "Product ID: B073VRRZV3\n",
            "Brand Name: Unknown\n",
            "Product Name: IndusDiva Green Nuapatna Cotton Handloom Saree\n",
            "Cosine Similarity: 0.9910445213317871\n",
            "____________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url'])\n",
        "\n",
        "# Step 1: Preprocess the product names (tokenization and lowercasing)\n",
        "def preprocess_text(text):\n",
        "    # Simple preprocessing: remove non-alphabetic characters and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Prepare data for Word2Vec\n",
        "product_names = data_cleaned['product_name'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Train Word2Vec model\n",
        "# Using the tokenized product names for Word2Vec training\n",
        "model = Word2Vec(sentences=product_names, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 4: Generate average vectors for each product name\n",
        "def get_average_vector(tokens, model):\n",
        "    # Get the vector of each word and calculate the average\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)  # If no words are in the model, return a zero vector\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Generate vectors for all product names\n",
        "product_vectors = np.array([get_average_vector(tokens, model) for tokens in product_names])\n",
        "\n",
        "# Recommender System Function\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on textual similarity using Word2Vec.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Product vectors (Word2Vec)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Compute pairwise cosine similarity\n",
        "        cosine_sim = cosine_similarity([vectors[p_id]], vectors)[0]\n",
        "        # Sort distances (higher cosine similarity = more similar)\n",
        "        indices = np.argsort(cosine_sim)[::-1][1:numProduct + 1]  # Skip self (first index)\n",
        "        p_sim = np.sort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "\n",
        "        similar_products = list(indices)\n",
        "\n",
        "        for i, idx in enumerate(similar_products):\n",
        "            # Display product image and details\n",
        "            display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "            print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "            print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "            print(f\"Brand Name: {dataset['brand_name'].iloc[idx] if not pd.isna(dataset['brand_name'].iloc[idx]) else 'Unknown'}\")\n",
        "            print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "            print(f\"Cosine Similarity: {p_sim[i]}\")\n",
        "            print(\"_\" * 60)\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage: Pass a valid p_id (row index of a product in the cleaned DataFrame)\n",
        "RecommenderSystems(0, 5, product_vectors, data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43468f08-996c-4549-ac99-56846fee1d89",
      "metadata": {
        "id": "43468f08-996c-4549-ac99-56846fee1d89",
        "outputId": "731f3365-af7f-46e7-ef2b-a68ced6aa86f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Cosine Similarity: 1.0\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/810yHP4dVtL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91Lk9W9EuoL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 2d249ce6637640fe9ef8a5cf20cbc9dc\n",
            "Product ID: B07W5X5BN4\n",
            "Brand Name: La' Facon\n",
            "Product Name: Cotton Kalamkari Handblock Saree Blouse/Kurti Fabric 100 cms Purple Colour - Budda Print\n",
            "Cosine Similarity: 0.9536398466283332\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/51Mwil2ULiL._UL1280_.jpg|https://images-na.ssl-images-amazon.com/images/I/51Mwil2ULiL._UL1280_.jpg|https://images-na.ssl-images-amazon.com/images/I/51Mwil2ULiL._UL1280_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: ca789f3bfdd58177c8803e832c38c857\n",
            "Product ID: B07TZ823HV\n",
            "Brand Name: SAMRUDDHI TRENDZ\n",
            "Product Name: SAMRUDDHI TRENDZ Women's Plain Solid Georgette Saree with Blouse Piece (Red)\n",
            "Cosine Similarity: 0.9471272016316458\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/91aYJy7KXNL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91BJ-ILbw9L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/51mgiJIoS6L._UL1100_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 79113d9c86e89ee3ec144cdedd06a8dd\n",
            "Product ID: B07NZTJJPD\n",
            "Brand Name: Unknown\n",
            "Product Name: Maliqua Plain Weave Poly Cotton Printed Saree with Blouse Piece (Grey & Black)\n",
            "Cosine Similarity: 0.9461291801607525\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81BIa7a8l7L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71At4joDRCL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/714s66XNwwL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71plVM5FjXL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: ad403242ebe347eeba10ab8712ac527c\n",
            "Product ID: B07BW839TC\n",
            "Brand Name: DHRUVI TRENDZ\n",
            "Product Name: Dhruvi Trendz Women's Cotton Silk Solid Saree With Blouse Piece Material (Green)\n",
            "Cosine Similarity: 0.9460353780815213\n",
            "____________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url'])\n",
        "\n",
        "# Step 1: Preprocess the product names (tokenization and lowercasing)\n",
        "def preprocess_text(text):\n",
        "    # Simple preprocessing: remove non-alphabetic characters and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Prepare data for Word2Vec\n",
        "product_names = data_cleaned['product_name'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Train Word2Vec model\n",
        "# Using the tokenized product names for Word2Vec training\n",
        "word2vec_model = Word2Vec(sentences=product_names, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 4: Generate average vectors for each product name using Word2Vec\n",
        "def get_average_vector(tokens, model):\n",
        "    # Get the vector of each word and calculate the average\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)  # If no words are in the model, return a zero vector\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Generate Word2Vec vectors for all product names\n",
        "word2vec_vectors = np.array([get_average_vector(tokens, word2vec_model) for tokens in product_names])\n",
        "\n",
        "# Step 5: Generate TF-IDF vectors\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vect.fit_transform(data_cleaned['product_name'])\n",
        "\n",
        "# Step 6: Combine TF-IDF and Word2Vec vectors in a memory-efficient way (use sparse matrices)\n",
        "def combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors):\n",
        "    # Convert Word2Vec vectors to a sparse matrix\n",
        "    word2vec_sparse = csr_matrix(word2vec_vectors, dtype=np.float32)  # Convert to sparse format (float32 for memory efficiency)\n",
        "\n",
        "    # Concatenate the sparse TF-IDF matrix with Word2Vec vectors\n",
        "    combined_vectors = hstack([tfidf_matrix, word2vec_sparse])\n",
        "\n",
        "    return combined_vectors\n",
        "\n",
        "# Get the combined vectors (sparse matrix)\n",
        "combined_vectors = combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors)\n",
        "\n",
        "# Recommender System Function\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on hybrid similarity using TF-IDF + Word2Vec.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Combined TF-IDF + Word2Vec vectors (sparse matrix)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Compute pairwise cosine similarity\n",
        "        cosine_sim = cosine_similarity(vectors[p_id], vectors).flatten()  # Cosine similarity\n",
        "        # Sort distances (higher cosine similarity = more similar)\n",
        "        indices = np.argsort(cosine_sim)[::-1][1:numProduct + 1]  # Skip self (first index)\n",
        "        p_sim = np.sort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "\n",
        "        similar_products = list(indices)\n",
        "\n",
        "        for i, idx in enumerate(similar_products):\n",
        "            # Display product image and details\n",
        "            display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "            print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "            print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "            print(f\"Brand Name: {dataset['brand_name'].iloc[idx] if not pd.isna(dataset['brand_name'].iloc[idx]) else 'Unknown'}\")\n",
        "            print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "            print(f\"Cosine Similarity: {p_sim[i]}\")\n",
        "            print(\"_\" * 60)\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage: Pass a valid p_id (row index of a product in the cleaned DataFrame)\n",
        "RecommenderSystems(0, 5, combined_vectors, data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87f5a3f-e94f-4f3a-8460-72e1cc29a259",
      "metadata": {
        "id": "a87f5a3f-e94f-4f3a-8460-72e1cc29a259",
        "outputId": "8fc61e96-0f7c-4da2-c29f-a8289b67c353"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Cosine Similarity: 1.0000000000000004\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/810yHP4dVtL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91Lk9W9EuoL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 2d249ce6637640fe9ef8a5cf20cbc9dc\n",
            "Product ID: B07W5X5BN4\n",
            "Brand Name: La' Facon\n",
            "Product Name: Cotton Kalamkari Handblock Saree Blouse/Kurti Fabric 100 cms Purple Colour - Budda Print\n",
            "Cosine Similarity: 0.9552630702951231\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/91aYJy7KXNL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91BJ-ILbw9L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/51mgiJIoS6L._UL1100_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 79113d9c86e89ee3ec144cdedd06a8dd\n",
            "Product ID: B07NZTJJPD\n",
            "Brand Name: Unknown\n",
            "Product Name: Maliqua Plain Weave Poly Cotton Printed Saree with Blouse Piece (Grey & Black)\n",
            "Cosine Similarity: 0.9475652861671382\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MpCZL%2BXyL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81WKjmByqFL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81KbD6UMcGL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/713jToMsUdL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A18JTW8pjjL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1RBvDRgikL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1OU9KfJbCL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: fe53377eaf34a302e67bb95c6659de1a\n",
            "Product ID: B07WSWCD4F\n",
            "Brand Name: Viva N Diva\n",
            "Product Name: Viva N Diva Light Pink Poly Cotton Blend Printed Saree with Blouse\n",
            "Cosine Similarity: 0.9466408978087233\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/71WVOSdNTvL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71KgbhJ2xCL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81qKjOnTHkL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81HeWea-wPL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1UmBw3OgrL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A14%2B7lx9PWL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1GP0J5SJtL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: dd5fb34f1242121aeff67c7d068b43fe\n",
            "Product ID: B07WNWLLWV\n",
            "Brand Name: Viva N Diva\n",
            "Product Name: Viva N Diva Off White Poly Cotton Blend Printed Saree with Blouse\n",
            "Cosine Similarity: 0.9465125596734599\n",
            "____________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url'])\n",
        "\n",
        "# Step 1: Preprocess the product names (tokenization and lowercasing)\n",
        "def preprocess_text(text):\n",
        "    # Simple preprocessing: remove non-alphabetic characters and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Prepare data for Word2Vec\n",
        "product_names = data_cleaned['product_name'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Train Word2Vec model\n",
        "# Using the tokenized product names for Word2Vec training\n",
        "word2vec_model = Word2Vec(sentences=product_names, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 4: Generate average vectors for each product name using Word2Vec\n",
        "def get_average_vector(tokens, model):\n",
        "    # Get the vector of each word and calculate the average\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)  # If no words are in the model, return a zero vector\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Generate Word2Vec vectors for all product names\n",
        "word2vec_vectors = np.array([get_average_vector(tokens, word2vec_model) for tokens in product_names])\n",
        "\n",
        "# Step 5: Generate TF-IDF vectors\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vect.fit_transform(data_cleaned['product_name'])\n",
        "\n",
        "# Step 6: Combine TF-IDF and Word2Vec vectors in a memory-efficient way (use sparse matrices)\n",
        "def combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors):\n",
        "    # Convert Word2Vec vectors to a sparse matrix\n",
        "    word2vec_sparse = csr_matrix(word2vec_vectors, dtype=np.float32)  # Convert to sparse format (float32 for memory efficiency)\n",
        "\n",
        "    # Concatenate the sparse TF-IDF matrix with Word2Vec vectors\n",
        "    combined_vectors = hstack([tfidf_matrix, word2vec_sparse])\n",
        "\n",
        "    return combined_vectors\n",
        "\n",
        "# Get the combined vectors (sparse matrix)\n",
        "combined_vectors = combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors)\n",
        "\n",
        "# Recommender System Function with Brand-Based Filtering\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on hybrid similarity using TF-IDF + Word2Vec and brand filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Combined TF-IDF + Word2Vec vectors (sparse matrix)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Get the brand of the input product\n",
        "        input_brand = dataset['brand_name'].iloc[p_id] if not pd.isna(dataset['brand_name'].iloc[p_id]) else 'Unknown'\n",
        "\n",
        "        # Compute pairwise cosine similarity (cosine similarity between product vectors)\n",
        "        cosine_sim = cosine_similarity(vectors[p_id], vectors).flatten()  # Cosine similarity\n",
        "        # Sort distances (higher cosine similarity = more similar)\n",
        "        indices = np.argsort(cosine_sim)[::-1][1:numProduct + 1]  # Skip self (first index)\n",
        "        p_sim = np.sort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "\n",
        "        # Filter out products that don't match the same brand (if any) - prioritizing same-brand products\n",
        "        same_brand_indices = [idx for idx in indices if dataset['brand_name'].iloc[idx] == input_brand]\n",
        "\n",
        "        # If no same-brand products, add products from other brands\n",
        "        if len(same_brand_indices) < numProduct:\n",
        "            other_brand_indices = [idx for idx in indices if dataset['brand_name'].iloc[idx] != input_brand]\n",
        "            # Combine same brand and other brand indices\n",
        "            final_indices = same_brand_indices + other_brand_indices[:(numProduct - len(same_brand_indices))]\n",
        "        else:\n",
        "            final_indices = same_brand_indices[:numProduct]\n",
        "\n",
        "        # Display the recommended products\n",
        "        for i, idx in enumerate(final_indices):\n",
        "            # Display product image and details\n",
        "            display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "            print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "            print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "            print(f\"Brand Name: {dataset['brand_name'].iloc[idx] if not pd.isna(dataset['brand_name'].iloc[idx]) else 'Unknown'}\")\n",
        "            print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "            print(f\"Cosine Similarity: {p_sim[np.where(indices == idx)[0][0]]}\")  # Getting the similarity for the selected product\n",
        "            print(\"_\" * 60)\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage: Pass a valid p_id (row index of a product in the cleaned DataFrame)\n",
        "RecommenderSystems(0, 5, combined_vectors, data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5409d9e7-b5c6-4995-88ce-7ca9228cc99f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "5409d9e7-b5c6-4995-88ce-7ca9228cc99f",
        "outputId": "adc6b448-5842-4cf7-c8d1-1e2446ac775b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-6c8f712784d4>:28: DtypeWarning: Columns (31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'image_url'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_url'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6c8f712784d4>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_first_valid_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mdata_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'product_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Limit to the first 15 images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_url'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input as preprocess_input_vgg19\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input as preprocess_input_mobilenet\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Concatenate, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to fetch and preprocess an image from a URL\n",
        "def fetch_image(url, target_size=(224, 224)):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        img = img.resize(target_size)\n",
        "        img_array = np.array(img)\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching image from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/fashion_products_data.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract valid URLs and clean the data\n",
        "def extract_first_valid_url(concatenated_urls):\n",
        "    if isinstance(concatenated_urls, str):\n",
        "        urls = concatenated_urls.split('|')\n",
        "        for url in urls:\n",
        "            if url.startswith(\"http\"):\n",
        "                return url\n",
        "    return None\n",
        "\n",
        "data['image_url'] = data['image_url'].apply(extract_first_valid_url)\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url']).iloc[:15]  # Limit to the first 15 images\n",
        "\n",
        "# Fetch and preprocess the images\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "print(\"Fetching and preprocessing images...\")\n",
        "for idx, row in tqdm(data_cleaned.iterrows(), total=len(data_cleaned)):\n",
        "    img_array = fetch_image(row['image_url'])\n",
        "    if img_array is not None:\n",
        "        images.append(img_array)\n",
        "        labels.append(idx)\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Train-test split (80%-20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Definition\n",
        "def create_model(input_shape):\n",
        "    # Input layers for both branches\n",
        "    input_layer_vgg = Input(shape=input_shape)\n",
        "    input_layer_mobilenet = Input(shape=input_shape)\n",
        "\n",
        "    # VGG19 branch\n",
        "    vgg19_preprocessed = preprocess_input_vgg19(input_layer_vgg)\n",
        "    vgg19_base = VGG19(weights='imagenet', include_top=False, input_tensor=vgg19_preprocessed)\n",
        "    vgg19_base.trainable = False\n",
        "    vgg19_output = GlobalAveragePooling2D()(vgg19_base.output)\n",
        "\n",
        "    # MobileNetV2 branch\n",
        "    mobilenet_preprocessed = preprocess_input_mobilenet(input_layer_mobilenet)\n",
        "    mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=mobilenet_preprocessed, input_shape=input_shape)\n",
        "    mobilenet_base.trainable = False\n",
        "    mobilenet_output = GlobalAveragePooling2D()(mobilenet_base.output)\n",
        "\n",
        "    # Combine branches\n",
        "    combined_output = Concatenate()([vgg19_output, mobilenet_output])\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dropout(0.5)(combined_output)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    output = Dense(len(np.unique(labels)), activation='softmax')(x)\n",
        "\n",
        "    # Create final model\n",
        "    model = Model(inputs=[input_layer_vgg, input_layer_mobilenet], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Preprocess inputs for each branch\n",
        "X_train_vgg = preprocess_input_vgg19(X_train)\n",
        "X_train_mobilenet = preprocess_input_mobilenet(X_train)\n",
        "X_val_vgg = preprocess_input_vgg19(X_val)\n",
        "X_val_mobilenet = preprocess_input_mobilenet(X_val)\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_shape=(224, 224, 3))\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "model.fit([X_train_vgg, X_train_mobilenet], y_train, validation_data=([X_val_vgg, X_val_mobilenet], y_val), epochs=5, batch_size=4, verbose=1)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('hybrid_model_vgg19_mobilenet.h5')\n",
        "print(\"Model saved as 'hybrid_model_vgg19_mobilenet.h5'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32d333c4-3b58-425d-b9a8-bebe15e77a06",
      "metadata": {
        "id": "32d333c4-3b58-425d-b9a8-bebe15e77a06",
        "outputId": "1e4f3102-429c-48fe-976f-1abe89e323ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching and preprocessing images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 15/15 [00:06<00:00,  2.20it/s]\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13900\\1523335232.py:78: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=mobilenet_preprocessed)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Epoch 1/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4s/step - accuracy: 0.0000e+00 - loss: 9.1440 - val_accuracy: 0.0000e+00 - val_loss: 9.1888\n",
            "Epoch 2/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 7.7020 - val_accuracy: 0.0000e+00 - val_loss: 12.5458\n",
            "Epoch 3/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.7083 - loss: 1.8790 - val_accuracy: 0.0000e+00 - val_loss: 16.1193\n",
            "Epoch 4/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2.4343 - val_accuracy: 0.0000e+00 - val_loss: 17.6217\n",
            "Epoch 5/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5938 - loss: 1.4421 - val_accuracy: 0.0000e+00 - val_loss: 18.5897\n",
            "Model saved in TensorFlow format as 'hybrid_model_vgg19_mobilenet.keras'.\n",
            "Error fetching image from https://example.com/path/to/your/image.jpg: cannot identify image file <_io.BytesIO object at 0x00000147A0781990>\n",
            "Recommended items based on the provided image:\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input as preprocess_input_vgg19\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input as preprocess_input_mobilenet\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Concatenate, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import ssl\n",
        "\n",
        "# Optional SSL bypass (only use if you face SSL issues)\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Function to fetch and preprocess an image from a URL\n",
        "def fetch_image(url, target_size=(224, 224)):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        img = img.resize(target_size)\n",
        "        img_array = np.array(img)\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching image from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = 'C:/Users/admin/downloads/vector_database_schema.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract valid URLs and clean the data\n",
        "def extract_first_valid_url(concatenated_urls):\n",
        "    if isinstance(concatenated_urls, str):\n",
        "        urls = concatenated_urls.split('|')\n",
        "        for url in urls:\n",
        "            if url.startswith(\"http\"):\n",
        "                return url\n",
        "    return None\n",
        "\n",
        "data['image_url'] = data['image_url'].apply(extract_first_valid_url)\n",
        "data_cleaned = data.dropna(subset=['product_name', 'image_url']).iloc[:15]  # Limit to the first 15 images\n",
        "\n",
        "# Fetch and preprocess the images\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "print(\"Fetching and preprocessing images...\")\n",
        "for idx, row in tqdm(data_cleaned.iterrows(), total=len(data_cleaned)):\n",
        "    img_array = fetch_image(row['image_url'])\n",
        "    if img_array is not None:\n",
        "        images.append(img_array)\n",
        "        labels.append(idx)\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Train-test split (80%-20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Definition\n",
        "def create_model(input_shape):\n",
        "    # Input layers for both branches\n",
        "    input_layer_vgg = Input(shape=input_shape)\n",
        "    input_layer_mobilenet = Input(shape=input_shape)\n",
        "\n",
        "    # VGG19 branch\n",
        "    vgg19_preprocessed = preprocess_input_vgg19(input_layer_vgg)\n",
        "    vgg19_base = VGG19(weights='imagenet', include_top=False, input_tensor=vgg19_preprocessed)\n",
        "    vgg19_base.trainable = False\n",
        "    vgg19_output = GlobalAveragePooling2D()(vgg19_base.output)\n",
        "\n",
        "    # MobileNetV2 branch\n",
        "    mobilenet_preprocessed = preprocess_input_mobilenet(input_layer_mobilenet)\n",
        "    mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=mobilenet_preprocessed)\n",
        "    mobilenet_base.trainable = False\n",
        "    mobilenet_output = GlobalAveragePooling2D()(mobilenet_base.output)\n",
        "\n",
        "    # Combine branches\n",
        "    combined_output = Concatenate()([vgg19_output, mobilenet_output])\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dropout(0.5)(combined_output)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    output = Dense(len(np.unique(labels)), activation='softmax')(x)\n",
        "\n",
        "    # Create final model\n",
        "    model = Model(inputs=[input_layer_vgg, input_layer_mobilenet], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Preprocess inputs for each branch\n",
        "X_train_vgg = preprocess_input_vgg19(X_train)\n",
        "X_train_mobilenet = preprocess_input_mobilenet(X_train)\n",
        "X_val_vgg = preprocess_input_vgg19(X_val)\n",
        "X_val_mobilenet = preprocess_input_mobilenet(X_val)\n",
        "\n",
        "# Create the model\n",
        "model = create_model(input_shape=(224, 224, 3))\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "model.fit([X_train_vgg, X_train_mobilenet], y_train, validation_data=([X_val_vgg, X_val_mobilenet], y_val), epochs=5, batch_size=4, verbose=1)\n",
        "\n",
        "# Save the model in TensorFlow format\n",
        "model.save('hybrid_model_vgg19_mobilenet.keras')\n",
        "print(\"Model saved in TensorFlow format as 'hybrid_model_vgg19_mobilenet.keras'.\")\n",
        "\n",
        "# Loading the trained model for inference\n",
        "model = tf.keras.models.load_model('hybrid_model_vgg19_mobilenet.keras')\n",
        "\n",
        "# Recommendation System Function\n",
        "def make_recommendation(image_url, model, dataset, top_n=3):\n",
        "    img_array = fetch_image(image_url)  # Fetch the new image\n",
        "    if img_array is None:\n",
        "        return None\n",
        "\n",
        "    img_array_vgg = preprocess_input_vgg19(np.expand_dims(img_array, axis=0))\n",
        "    img_array_mobilenet = preprocess_input_mobilenet(np.expand_dims(img_array, axis=0))\n",
        "\n",
        "    # Get prediction from the model\n",
        "    prediction = model.predict([img_array_vgg, img_array_mobilenet])\n",
        "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "    # Find the closest items based on predicted label\n",
        "    similar_items = dataset.iloc[np.where(labels == predicted_label)[0]]\n",
        "\n",
        "    # Return top N similar items\n",
        "    return similar_items.head(top_n)\n",
        "\n",
        "# Example usage for recommendations\n",
        "image_url = \"https://example.com/path/to/your/image.jpg\"  # Replace with an actual image URL\n",
        "recommended_items = make_recommendation(image_url, model, data_cleaned, top_n=3)\n",
        "print(\"Recommended items based on the provided image:\")\n",
        "print(recommended_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7600bbba-7bef-4628-878f-5fbeb2925584",
      "metadata": {
        "id": "7600bbba-7bef-4628-878f-5fbeb2925584",
        "outputId": "9b30d2f0-6d16-40f5-8fce-fc9b2744f08a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Images: 100%|| 10/10 [00:03<00:00,  3.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23s/step - accuracy: 0.3750 - loss: 2.3569 - val_accuracy: 0.0000e+00 - val_loss: 2.8263\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.6250 - loss: 1.4729 - val_accuracy: 0.0000e+00 - val_loss: 4.0392\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.6250 - loss: 1.1316 - val_accuracy: 0.0000e+00 - val_loss: 5.0322\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.8750 - loss: 0.5837 - val_accuracy: 0.0000e+00 - val_loss: 5.8146\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.1539 - val_accuracy: 0.0000e+00 - val_loss: 6.4705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as 'hybrid_model_vgg19_mobilenet.h5'.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import VGG19, MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Concatenate, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# --- Load and preprocess the dataset ---\n",
        "def preprocess_image(url, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Fetch and preprocess an image from a URL for model input.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        img = img.resize(target_size)\n",
        "        return img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load the dataset\n",
        "file_path =  'C:/Users/admin/downloads/vector_database_schema.csv'     # Update path as needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Filter valid rows and take the first 10 rows\n",
        "data_filtered = data.dropna(subset=['product_name', 'image_url']).head(10)\n",
        "data_filtered = data_filtered.reset_index(drop=True)\n",
        "\n",
        "# Prepare images and labels\n",
        "images = []\n",
        "labels = []\n",
        "for _, row in tqdm(data_filtered.iterrows(), total=len(data_filtered), desc=\"Processing Images\"):\n",
        "    img_array = preprocess_image(row['image_url'])\n",
        "    if img_array is not None:\n",
        "        images.append(img_array)\n",
        "        labels.append(row['product_id'])\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "unique_labels = np.unique(labels)\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "labels = np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "# One-hot encode labels\n",
        "labels_onehot = to_categorical(labels, num_classes=len(unique_labels))\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Build Hybrid Model ---\n",
        "input_shape = (224, 224, 3)\n",
        "vgg19_base = VGG19(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
        "mobilenet_base = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Freeze base models\n",
        "vgg19_base.trainable = False\n",
        "mobilenet_base.trainable = False\n",
        "\n",
        "# Combine VGG19 and MobileNet features\n",
        "input_layer = Input(shape=input_shape)\n",
        "vgg19_features = vgg19_base(input_layer)\n",
        "mobilenet_features = mobilenet_base(input_layer)\n",
        "\n",
        "# Pooling layers\n",
        "vgg19_output = GlobalAveragePooling2D()(vgg19_features)\n",
        "mobilenet_output = GlobalAveragePooling2D()(mobilenet_features)\n",
        "\n",
        "# Concatenate and add dense layers\n",
        "combined = Concatenate()([vgg19_output, mobilenet_output])\n",
        "combined = Dropout(0.5)(combined)\n",
        "combined = Dense(512, activation=\"relu\")(combined)\n",
        "output_layer = Dense(len(unique_labels), activation=\"softmax\")(combined)\n",
        "\n",
        "# Create and compile the model\n",
        "hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "hybrid_model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                     loss=\"categorical_crossentropy\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "hybrid_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=16, verbose=1)\n",
        "\n",
        "# Save the model\n",
        "hybrid_model.save('hybrid_model_vgg19_mobilenet.h5')\n",
        "print(\"Model saved as 'hybrid_model_vgg19_mobilenet.h5'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b89fb8-e501-4740-a2e7-12bcd0515029",
      "metadata": {
        "id": "c9b89fb8-e501-4740-a2e7-12bcd0515029",
        "outputId": "297b9681-3dd6-4fff-d893-8d02d25dd6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "Recommendations for Most Similar Product: Sf Jeans By Pantaloons Men's Plain Slim fit T-Shirt\n",
            "\n",
            "Product Name: People Men's Printed Regular fit T-Shirt\n",
            "Similarity Score: 0.7569\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/91NwCUxcFXL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91iAuw5G8fL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91QarLmd1FL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81SteSIGusL._UL1500_.jpg\" width=\"150\" height=\"150\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Product Name: PrintOctopus Graphic Printed T-Shirt for Men Chill T-Shirt | Hindi Quote T-Shirt | Half Sleeve T-Shirt for Women | Round Neck T Shirt | 100% Cotton T-Shirt | Short Sleeve T Shirt\n",
            "Similarity Score: 0.7130\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/61-jVp8ZpCL._UL1200_.jpg|https://images-na.ssl-images-amazon.com/images/I/614e0lWSa9L._UL1200_.jpg|https://images-na.ssl-images-amazon.com/images/I/61yVz-aQggL._UL1100_.jpg|https://images-na.ssl-images-amazon.com/images/I/81SqN9XzxfL._UL1200_.jpg|https://images-na.ssl-images-amazon.com/images/I/61Y-2D0dnzL._UL1200_.jpg\" width=\"150\" height=\"150\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Product Name: Monte Carlo Grey Solid Cotton Blend Polo Collar Tracksuit\n",
            "Similarity Score: 0.6725\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/61F0N7uUiLL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61K0oYhFa4L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61sHst6HK6L._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61WItR6G6jL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91bzoyabeEL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/818lweeagxL._UL1500_.jpg\" width=\"150\" height=\"150\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from IPython.display import Image, display\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the trained model and remove the classification layers\n",
        "trained_model = hybrid_model\n",
        "feature_extractor = Model(inputs=trained_model.input, outputs=trained_model.layers[-3].output)  # Get the second-to-last layer\n",
        "\n",
        "# Extract features for all images in the dataset\n",
        "features = feature_extractor.predict(images)\n",
        "\n",
        "# Function to get the most similar product based on cosine similarity\n",
        "def find_most_similar_product():\n",
        "    \"\"\"\n",
        "    Find the product with the most similar images to the rest of the dataset.\n",
        "    \"\"\"\n",
        "    average_similarities = []\n",
        "\n",
        "    # Calculate average similarity for each product\n",
        "    for i in range(len(data_filtered)):\n",
        "        product_feature = features[i].reshape(1, -1)\n",
        "        similarities = cosine_similarity(product_feature, features).flatten()\n",
        "\n",
        "        # Exclude the similarity with itself (i.e., the diagonal value)\n",
        "        avg_similarity = np.mean([similarities[j] for j in range(len(similarities)) if j != i])\n",
        "        average_similarities.append(avg_similarity)\n",
        "\n",
        "    # Get the index of the product with the highest average similarity\n",
        "    most_similar_idx = np.argmax(average_similarities)\n",
        "    return most_similar_idx\n",
        "\n",
        "# Recommendation function based on the most similar product\n",
        "def recommend_images_for_most_similar_product(top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend images similar to the most similar product.\n",
        "    \"\"\"\n",
        "    most_similar_idx = find_most_similar_product()\n",
        "    input_feature = features[most_similar_idx].reshape(1, -1)\n",
        "    similarities = cosine_similarity(input_feature, features).flatten()\n",
        "\n",
        "    # Get top N recommendations (excluding the input itself)\n",
        "    top_indices = np.argsort(similarities)[::-1][1:top_n + 1]\n",
        "\n",
        "    print(f\"Recommendations for Most Similar Product: {data_filtered.iloc[most_similar_idx]['product_name']}\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"\\nProduct Name: {data_filtered.iloc[idx]['product_name']}\")\n",
        "        print(f\"Similarity Score: {similarities[idx]:.4f}\")\n",
        "        display(Image(url=data_filtered.iloc[idx]['image_url'], width=150, height=150))\n",
        "\n",
        "# Example usage: Recommend similar images for the most similar product\n",
        "recommend_images_for_most_similar_product(top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003346d1-58ca-4a8d-aa04-5d66e5a19214",
      "metadata": {
        "id": "003346d1-58ca-4a8d-aa04-5d66e5a19214",
        "outputId": "ad8e8bd8-4197-477f-8f8b-e8b36aedb967"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15632\\2573169467.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_cleaned['combined_name'] = data_cleaned['product_name'] + ' ' + data_cleaned['brand_name']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Cosine Similarity: 1.0000\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/810yHP4dVtL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91Lk9W9EuoL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 2d249ce6637640fe9ef8a5cf20cbc9dc\n",
            "Product ID: B07W5X5BN4\n",
            "Brand Name: La' Facon\n",
            "Product Name: Cotton Kalamkari Handblock Saree Blouse/Kurti Fabric 100 cms Purple Colour - Budda Print\n",
            "Cosine Similarity: 0.9691\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/71WVOSdNTvL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/71KgbhJ2xCL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81qKjOnTHkL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81HeWea-wPL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1UmBw3OgrL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A14%2B7lx9PWL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A1GP0J5SJtL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: dd5fb34f1242121aeff67c7d068b43fe\n",
            "Product ID: B07WNWLLWV\n",
            "Brand Name: Viva N Diva\n",
            "Product Name: Viva N Diva Off White Poly Cotton Blend Printed Saree with Blouse\n",
            "Cosine Similarity: 0.9464\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/91brDToRbKL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91LRz6hrXhL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91VzsSrTAmL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/A19QdF4nwRL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91aSRRY71ML._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91e302O44PL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/61DrY3KHsQL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: a6ab8e30772c5bb06abcdb41225e80db\n",
            "Product ID: B07SKWD71D\n",
            "Brand Name: SARVADARSHI FASHION\n",
            "Product Name: Sarvadarshi Fashion Presents Womens Letest Embroidered Half & Half Satin Silk & Silk Fabric With Blouse Piece Saree\n",
            "Cosine Similarity: 0.9453\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/91KFDCR6sUL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91fDgPTq5NL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91QgV7KaZFL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/91Gakk0l6BL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 27a02a5158f4c0af989196440c960d76\n",
            "Product ID: B07VH5BRFH\n",
            "Brand Name: The Fashion Outlets\n",
            "Product Name: The Fashion Outlets Women's Heavy Silk Digital Print Jacquard Saree with Blouse (Peach)\n",
            "Cosine Similarity: 0.9444\n",
            "____________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/Admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name, brand_name, or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'brand_name', 'image_url'])\n",
        "\n",
        "# Step 1: Preprocess text fields (product_name and brand_name)\n",
        "def preprocess_text(text):\n",
        "    # Simple preprocessing: remove non-alphabetic characters and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Combine product_name and brand_name\n",
        "data_cleaned['combined_name'] = data_cleaned['product_name'] + ' ' + data_cleaned['brand_name']\n",
        "combined_names = data_cleaned['combined_name'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Train Word2Vec model on combined names\n",
        "word2vec_model = Word2Vec(sentences=combined_names, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 4: Generate average vectors for combined names\n",
        "def get_average_vector(tokens, model):\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "word2vec_vectors = np.array([get_average_vector(tokens, word2vec_model) for tokens in combined_names])\n",
        "\n",
        "# Step 5: Generate TF-IDF vectors for combined names\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vect.fit_transform(data_cleaned['combined_name'])\n",
        "\n",
        "# Step 6: Combine TF-IDF and Word2Vec vectors\n",
        "def combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors):\n",
        "    word2vec_sparse = csr_matrix(word2vec_vectors, dtype=np.float32)\n",
        "    combined_vectors = hstack([tfidf_matrix, word2vec_sparse])\n",
        "    return combined_vectors\n",
        "\n",
        "combined_vectors = combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors)\n",
        "\n",
        "# Updated Recommender System\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on hybrid similarity using TF-IDF + Word2Vec and brand filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Combined TF-IDF + Word2Vec vectors (sparse matrix)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Compute cosine similarity\n",
        "        cosine_sim = cosine_similarity(vectors[p_id], vectors).flatten()\n",
        "\n",
        "        # Sort similarities and get top indices\n",
        "        indices = np.argsort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "        p_sim = np.sort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "\n",
        "        # Display recommended products\n",
        "        for i, idx in enumerate(indices):\n",
        "            display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "            print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "            print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "            print(f\"Brand Name: {dataset['brand_name'].iloc[idx]}\")\n",
        "            print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "            print(f\"Cosine Similarity: {p_sim[i]:.4f}\")\n",
        "            print(\"_\" * 60)\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage: Pass a valid p_id (row index of a product in the cleaned DataFrame)\n",
        "RecommenderSystems(0, 5, combined_vectors, data_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5914f363-df14-4da2-aacd-ef224fb72110",
      "metadata": {
        "id": "5914f363-df14-4da2-aacd-ef224fb72110",
        "outputId": "876af877-11f5-45e4-9d39-62fbd63306f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4516\\1971002351.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_cleaned.loc[:, 'combined_name'] = data_cleaned['product_name'] + ' ' + data_cleaned['brand_name']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_4516\\1971002351.py:65: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  product_mask = dataset['product_name'].str.contains(reference_product, case=False, na=False)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/admin/downloads/vector_database_schema.csv'  # Replace with your file path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name, brand_name, or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'brand_name', 'image_url'])\n",
        "\n",
        "# Step 1: Preprocess text fields (product_name and brand_name)\n",
        "def preprocess_text(text):\n",
        "    # Simple preprocessing: remove non-alphabetic characters and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Combine product_name and brand_name\n",
        "data_cleaned.loc[:, 'combined_name'] = data_cleaned['product_name'] + ' ' + data_cleaned['brand_name']\n",
        "combined_names = data_cleaned['combined_name'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Train Word2Vec model on combined names\n",
        "word2vec_model = Word2Vec(sentences=combined_names, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 4: Generate average vectors for combined names\n",
        "def get_average_vector(tokens, model):\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "word2vec_vectors = np.array([get_average_vector(tokens, word2vec_model) for tokens in combined_names])\n",
        "\n",
        "# Step 5: Generate TF-IDF vectors for combined names\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vect.fit_transform(data_cleaned['combined_name'])\n",
        "\n",
        "# Step 6: Combine TF-IDF and Word2Vec vectors\n",
        "def combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors):\n",
        "    word2vec_sparse = csr_matrix(word2vec_vectors, dtype=np.float32)\n",
        "    combined_vectors = hstack([tfidf_matrix, word2vec_sparse])\n",
        "    return combined_vectors\n",
        "\n",
        "combined_vectors = combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors)\n",
        "\n",
        "# Recommender System with filtering\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset, filter_by_brand=True, filter_by_product=True):\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        reference_brand = dataset['brand_name'].iloc[p_id].lower()\n",
        "        reference_product = dataset['product_name'].iloc[p_id].lower()\n",
        "\n",
        "        cosine_sim = cosine_similarity(vectors[p_id], vectors).flatten()\n",
        "\n",
        "        if filter_by_brand:\n",
        "            brand_mask = dataset['brand_name'].str.lower() == reference_brand\n",
        "        else:\n",
        "            brand_mask = np.ones(len(dataset), dtype=bool)\n",
        "\n",
        "        if filter_by_product:\n",
        "            product_mask = dataset['product_name'].str.contains(reference_product, case=False, na=False)\n",
        "        else:\n",
        "            product_mask = np.ones(len(dataset), dtype=bool)\n",
        "\n",
        "        combined_mask = brand_mask & product_mask\n",
        "        cosine_sim[~combined_mask] = -1\n",
        "\n",
        "        indices = np.argsort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "        p_sim = np.sort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            if cosine_sim[idx] > 0:\n",
        "                display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "                print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "                print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "                print(f\"Brand Name: {dataset['brand_name'].iloc[idx]}\")\n",
        "                print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "                print(f\"Cosine Similarity: {p_sim[i]:.4f}\")\n",
        "                print(\"_\" * 60)\n",
        "            else:\n",
        "                print(f\"No more valid recommendations found for filters.\")\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage\n",
        "RecommenderSystems(0, 5, combined_vectors, data_cleaned, filter_by_brand=True, filter_by_product=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3415859d-f3e5-4bd2-b830-42ecf1ea0b94",
      "metadata": {
        "id": "3415859d-f3e5-4bd2-b830-42ecf1ea0b94",
        "outputId": "e1ea756f-63ad-40aa-cbfa-0d96f7ff2c5f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://images-na.ssl-images-amazon.com/images/I/81MqmouZ9kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/814Tnuvt5kL._UL1500_.jpg|https://images-na.ssl-images-amazon.com/images/I/81fFr%2B%2Bd6TL._UL1500_.jpg\" width=\"100\" height=\"100\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID: 4188c32471f46876ed222dfcae360791\n",
            "Product ID: B07STS2W9T\n",
            "Brand Name: LA' Facon\n",
            "Product Name: LA' Facon Cotton Kalamkari Handblock Saree Blouse Fabric 100 cms Black Base Dancers (Cotton)\n",
            "Cosine Similarity: 1.0000\n",
            "____________________________________________________________\n",
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n",
            "No more valid recommendations found for filters.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'C:/Users/admin/downloads/vector_database_schema.csv'  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Clean the dataset by removing rows with missing product_name, brand_name, or image_url\n",
        "data_cleaned = data.dropna(subset=['product_name', 'brand_name', 'image_url']).copy()\n",
        "\n",
        "# Step 1: Preprocess text fields (product_name and brand_name)\n",
        "def preprocess_text(text):\n",
        "    # Simple preprocessing: remove non-alphabetic characters and lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Combine product_name and brand_name\n",
        "data_cleaned['combined_name'] = data_cleaned['product_name'] + ' ' + data_cleaned['brand_name']\n",
        "\n",
        "# Preprocess combined names\n",
        "combined_names = data_cleaned['combined_name'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Train Word2Vec model on combined names\n",
        "word2vec_model = Word2Vec(sentences=combined_names, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 4: Generate average vectors for combined names\n",
        "def get_average_vector(tokens, model):\n",
        "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "word2vec_vectors = np.array([get_average_vector(tokens, word2vec_model) for tokens in combined_names])\n",
        "\n",
        "# Step 5: Generate TF-IDF vectors for combined names\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vect.fit_transform(data_cleaned['combined_name'])\n",
        "\n",
        "# Step 6: Combine TF-IDF and Word2Vec vectors\n",
        "def combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors):\n",
        "    word2vec_sparse = csr_matrix(word2vec_vectors, dtype=np.float32)\n",
        "    combined_vectors = hstack([tfidf_matrix, word2vec_sparse])\n",
        "    return combined_vectors\n",
        "\n",
        "combined_vectors = combine_tfidf_word2vec(tfidf_matrix, word2vec_vectors)\n",
        "\n",
        "# Step 7: Define the Recommender System\n",
        "def RecommenderSystems(p_id, numProduct, vectors, dataset, filter_by_brand=True, filter_by_product=True):\n",
        "    \"\"\"\n",
        "    Recommend similar products based on hybrid similarity using TF-IDF + Word2Vec and optional brand/product filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - p_id: Index of the product in the DataFrame to find similar items for\n",
        "    - numProduct: Number of similar products to recommend\n",
        "    - vectors: Combined TF-IDF + Word2Vec vectors (sparse matrix)\n",
        "    - dataset: Cleaned DataFrame with required fields\n",
        "    - filter_by_brand: Boolean, if True, only recommend products with the same brand name\n",
        "    - filter_by_product: Boolean, if True, only recommend products with similar product names\n",
        "    \"\"\"\n",
        "    if 0 <= p_id < vectors.shape[0]:\n",
        "        # Get the brand name and product name of the reference product\n",
        "        reference_brand = dataset['brand_name'].iloc[p_id].lower()\n",
        "        reference_product = dataset['product_name'].iloc[p_id].lower()\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        cosine_sim = cosine_similarity(vectors[p_id], vectors).flatten()\n",
        "\n",
        "        # Apply filtering by brand name\n",
        "        if filter_by_brand:\n",
        "            brand_mask = dataset['brand_name'].str.lower() == reference_brand\n",
        "        else:\n",
        "            brand_mask = np.ones(len(dataset), dtype=bool)\n",
        "\n",
        "        # Apply filtering by product name\n",
        "        if filter_by_product:\n",
        "            product_mask = dataset['product_name'].str.contains(reference_product, case=False, na=False, regex=False)\n",
        "        else:\n",
        "            product_mask = np.ones(len(dataset), dtype=bool)\n",
        "\n",
        "        # Combine masks\n",
        "        combined_mask = brand_mask & product_mask\n",
        "\n",
        "        # Filter cosine similarity based on the combined mask\n",
        "        cosine_sim[~combined_mask] = -1  # Set similarity to -1 for products not matching the filters\n",
        "\n",
        "        # Sort similarities and get top indices\n",
        "        indices = np.argsort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "        p_sim = np.sort(cosine_sim)[::-1][1:numProduct + 1]\n",
        "\n",
        "        # Display recommended products\n",
        "        for i, idx in enumerate(indices):\n",
        "            if cosine_sim[idx] > 0:  # Only display products with valid similarity scores\n",
        "                display(Image(url=dataset['image_url'].iloc[idx], width=100, height=100))\n",
        "                print(f\"Unique ID: {dataset['unique_id'].iloc[idx]}\")\n",
        "                print(f\"Product ID: {dataset['product_id'].iloc[idx]}\")\n",
        "                print(f\"Brand Name: {dataset['brand_name'].iloc[idx]}\")\n",
        "                print(f\"Product Name: {dataset['product_name'].iloc[idx]}\")\n",
        "                print(f\"Cosine Similarity: {p_sim[i]:.4f}\")\n",
        "                print(\"_\" * 60)\n",
        "            else:\n",
        "                print(f\"No more valid recommendations found for filters.\")\n",
        "    else:\n",
        "        print(\"Error: p_id is out of range.\")\n",
        "\n",
        "# Example usage\n",
        "RecommenderSystems(0, 5, combined_vectors, data_cleaned, filter_by_brand=True, filter_by_product=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9f0a97-0e3f-475d-a925-c723645a5862",
      "metadata": {
        "id": "0c9f0a97-0e3f-475d-a925-c723645a5862"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}